---
# #####################################################################
# Deploy Rook
# #####################################################################
#
# Label master
#
- name: Kubernetes Label master as MDS Node
  become: true
  become_user: ubuntu
  k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Node
      metadata:
        name: "{{ hostvars[inventory_hostname]['ansible_hostname'] }}"
        labels:
          role: mds-node

## HELM DEPLOY OPERATOR
- name: Create Rook-Ceph Overrides File
  copy:
    dest: /home/ubuntu/overrides-rook-ceph.yml
    mode: 0640
    owner: ubuntu
    content: |
      # Default values for rook-ceph-operator
      # This is a YAML-formatted file.
      # Declare variables to be passed into your templates.

      image:
        prefix: rook
        repository: rook/ceph
        tag: {{ rook_ceph_image_tag }}
        pullPolicy: IfNotPresent

      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 256Mi

      nodeSelector:
      # Constraint rook-ceph-operator Deployment to nodes with label `disktype: ssd`.
      # For more info, see https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
      #  disktype: ssd

      #Â Tolerations for the rook-ceph-operator to allow it to run on nodes with particular taints
      tolerations: []

      # Delay to use in node.kubernetes.io/unreachable toleration
      unreachableNodeTolerationSeconds: 5

      # Whether rook watches its current namespace for CRDs or the entire cluster, defaults to false
      currentNamespaceOnly: false

      # Interval at which to get the ceph status and update the cluster custom resource status
      cephStatusCheckInterval: "60s"

      mon:
        healthCheckInterval: "45s"
        monOutTimeout: "600s"

      ## Annotations to be added to pod
      annotations: {}

      ## LogLevel can be set to: TRACE, DEBUG, INFO, NOTICE, WARNING, ERROR or CRITICAL
      logLevel: INFO

      ## If true, create & use RBAC resources
      ##
      rbacEnable: true

      ## If true, create & use PSP resources
      ##
      pspEnable: true

      ## Settings for whether to disable the drivers or other daemons if they are not
      ## needed
      csi:
        enableRbdDriver: true
        enableCephfsDriver: true
        enableGrpcMetrics: true
        enableSnapshotter: true
        # CSI CephFS plugin daemonset update strategy, supported values are OnDelete and RollingUpdate.
        # Default value is RollingUpdate.
        #rbdPluginUpdateStrategy: OnDelete
        # CSI Rbd plugin daemonset update strategy, supported values are OnDelete and RollingUpdate.
        # Default value is RollingUpdate.
        #cephFSPluginUpdateStrategy: OnDelete

        # Set provisonerTolerations and provisionerNodeAffinity for provisioner pod.
        # The CSI provisioner would be best to start on the same nodes as other ceph daemons.
        # provisionerTolerations:
        #    - key: key
        #      operator: Exists
        #      effect: NoSchedule
        # provisionerNodeAffinity: key1=value1,value2; key2=value3
        # Set pluginTolerations and pluginNodeAffinity for plugin daemonset pods.
        # The CSI plugins need to be started on all the nodes where the clients need to mount the storage.
        # pluginTolerations:
        #    - key: key
        #      operator: Exists
        #      effect: NoSchedule
        # pluginNodeAffinity: key1=value1,value2; key2=value3
        #cephfsGrpcMetricsPort: 9091
        #cephfsLivenessMetricsPort: 9081
        #rbdGrpcMetricsPort: 9090
        # Enable Ceph Kernel clients on kernel < 4.17. If your kernel does not support quotas for CephFS
        # you may want to disable this setting. However, this will cause an issue during upgrades
        # with the FUSE client. See the upgrade guide: https://rook.io/docs/rook/v1.2/ceph-upgrade.html
        forceCephFSKernelClient: true
        #rbdLivenessMetricsPort: 9080
        #kubeletDirPath: /var/lib/kubelet
        #cephcsi:
          #image: quay.io/cephcsi/cephcsi:v1.2.2
        #registrar:
          #image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0
        #provisioner:
          #image: quay.io/k8scsi/csi-provisioner:v1.4.0
        #snapshotter:
          #image: quay.io/k8scsi/csi-snapshotter:v1.2.2
        #attacher:
          #image: quay.io/k8scsi/csi-attacher:v1.2.0

      enableFlexDriver: true
      enableDiscoveryDaemon: true

      ## if true, run rook operator on the host network
      # useOperatorHostNetwork: true

      ## Rook Agent configuration
      ## toleration: NoSchedule, PreferNoSchedule or NoExecute
      ## tolerationKey: Set this to the specific key of the taint to tolerate
      ## tolerations: Array of tolerations in YAML format which will be added to agent deployment
      ## nodeAffinity: Set to labels of the node to match
      ## flexVolumeDirPath: The path where the Rook agent discovers the flex volume plugins
      ## libModulesDirPath: The path where the Rook agent can find kernel modules
      # agent:
      #   toleration: NoSchedule
      #   tolerationKey: key
      #   tolerations:
      #   - key: key
      #     operator: Exists
      #     effect: NoSchedule
      #   nodeAffinity: key1=value1,value2; key2=value3
      #   mountSecurityMode: Any
      ## For information on FlexVolume path, please refer to https://rook.io/docs/rook/master/flexvolume.html
      #   flexVolumeDirPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/
      #   libModulesDirPath: /lib/modules
      #   mounts: mount1=/host/path:/container/path,/host/path2:/container/path2

      ## Rook Discover configuration
      ## toleration: NoSchedule, PreferNoSchedule or NoExecute
      ## tolerationKey: Set this to the specific key of the taint to tolerate
      ## tolerations: Array of tolerations in YAML format which will be added to agent deployment
      ## nodeAffinity: Set to labels of the node to match
      # discover:
      #   toleration: NoSchedule
      #   tolerationKey: key
      #   tolerations:
      #   - key: key
      #     operator: Exists
      #     effect: NoSchedule
      #   nodeAffinity: key1=value1,value2; key2=value3

      # In some situations SELinux relabelling breaks (times out) on large filesystems, and doesn't work with cephfs ReadWriteMany volumes (last relabel wins).
      # Disable it here if you have similar issues.
      # For more details see https://github.com/rook/rook/issues/2417
      enableSelinuxRelabeling: true

      # Writing to the hostPath is required for the Ceph mon and osd pods. Given the restricted permissions in OpenShift with SELinux,
      # the pod must be running privileged in order to write to the hostPath volume, this must be set to true then.
      hostpathRequiresPrivileged: false

      # Disable automatic orchestration when new devices are discovered.
      disableDeviceHotplug: false

      # Blacklist certain disks according to the regex provided.
      discoverDaemonUdev:

      # imagePullSecrets option allow to pull docker images from private docker registry. Option will be passed to all service accounts.
      # imagePullSecrets:
      # - name: my-registry-secret

- name: Create Rook-Ceph Namespace
  become: true
  become_user: ubuntu
  shell: kubectl create namespace {{ rook_ceph_namespace }} --dry-run=true -o yaml | kubectl apply -f -
  args:
    chdir: $HOME

- name: Label Rook-Ceph Namespace to be ignored by Container Security
  become: true
  become_user: ubuntu
  shell: kubectl label ns {{ rook_ceph_namespace }} ignoreAdmissionControl=true --overwrite
  args:
    chdir: $HOME
  ignore_errors: true

- name: Install Rook Repo
  become: true
  become_user: ubuntu
  shell: helm repo add rook-release https://charts.rook.io/release

- name: Update Rook Repo
  become: true
  become_user: ubuntu
  shell: helm repo update

- name: Install Rook-Ceph
  become: true
  become_user: ubuntu
  shell: helm install --namespace {{ rook_ceph_namespace }} --values overrides-rook-ceph.yml {{ rook_ceph_name }} {{ rook_ceph_chart }} --version {{ rook_ceph_chart_version }} >> rook-ceph.log
  args:
    chdir: $HOME
    creates: rook-ceph.log

- name: Fetch rook-ceph.log
  become: true
  become_user: ubuntu
  fetch:
    src: /home/ubuntu/rook-ceph.log
    dest: ./site_{{ type }}/
    flat: yes
## HELM DEPLOY OPERATOR - END

## GIT DEPLOY OPERATOR
# - name: Rook-Ceph Create Directory
#   file:
#     path: /home/ubuntu/rook
#     state: directory
#     owner: ubuntu
#     mode: 0755
#
# - name: Rook-Ceph Checkout
#   become: true
#   become_user: ubuntu
#   git:
#     repo: "{{ rook_git }}"
#     dest: /home/ubuntu/rook
#     version: "{{ rook_version }}"
#     force: yes
#
# - name: pause
#   pause:
#     minutes: 10
#
# - name: Rook-Ceph Install Common
#   become: true
#   become_user: ubuntu
#   shell: kubectl apply -f /home/ubuntu/rook/cluster/examples/kubernetes/ceph/common.yaml --dry-run=true -o yaml | kubectl apply -f -
#   args:
#     chdir: $HOME
#
# - name: Rook-Ceph Enable Flex Driver in Operator
#   become: true
#   become_user: ubuntu
#   blockinfile:
#     path: /home/ubuntu/rook/cluster/examples/kubernetes/ceph/operator.yaml
#     insertafter: "^[\\s]*- name: ROOK_ENABLE_FLEX_DRIVER"
#     block: |
#           '          value: "true"'
#
# - name: Rook-Ceph Install Operator
#   become: true
#   become_user: ubuntu
#   shell: kubectl apply -f /home/ubuntu/rook/cluster/examples/kubernetes/ceph/operator.yaml --dry-run=true -o yaml | kubectl apply -f -
#   args:
#     chdir: $HOME
## GIT DEPLOY OPERATOR - END

- name: Create Rook-Ceph Cluster CRD
  copy:
    dest: /home/ubuntu/crd-rook-ceph-cluster.yml
    mode: 0640
    owner: ubuntu
    content: |
      apiVersion: ceph.rook.io/v1
      kind: CephCluster
      metadata:
        name: rook-ceph
        namespace: rook-ceph
      spec:
        cephVersion:
          # see the "Cluster Settings" section below for more details on which image of ceph to run
          image: {{ ceph_image }}
        dataDirHostPath: /var/lib/rook

        allowUnsupported: false
        skipUpgradeChecks: false
        dashboard:
          enabled: true
          ssl: true
        monitoring:
          enabled: false
          rulesNamespace: rook-ceph
        network:
          hostNetwork: false
        rbdMirroring:
          workers: 0

        mon:
          count: 3
          allowMultiplePerNode: true
        storage:
          useAllNodes: true
          useAllDevices: true
          # Cluster level list of directories to use for filestore-based OSD storage. This will create an OSD under the dataDirHostPath.
          # Will get deprecated and removed in upcoming Rook versions, sadly.
          directories:
            - path: /var/lib/rook

- name: Rook-Ceph Deploy Cluster
  become: true
  become_user: ubuntu
  shell: kubectl apply -f /home/ubuntu/crd-rook-ceph-cluster.yml --dry-run=true -o yaml | kubectl apply -f -
  args:
    chdir: $HOME

## Need to figure out how to use the csi provisioner
# - name: Create Rook-Ceph StorageClass CRD (Provisioner rook-ceph.cephfs.csi.ceph.com)
#   copy:
#     dest: /home/ubuntu/crd-rook-ceph-storageclass.yml
#     mode: 0640
#     owner: ubuntu
#     content: |
#       apiVersion: storage.k8s.io/v1
#       kind: StorageClass
#       metadata:
#         name: csi-cephfs
#       provisioner: rook-ceph.cephfs.csi.ceph.com
#       parameters:
#         # clusterID is the namespace where operator is deployed.
#         clusterID: rook-ceph
#
#         # CephFS filesystem name into which the volume shall be created
#         fsName: myfs
#
#         # Ceph pool into which the volume shall be created
#         # Required for provisionVolume: "true"
#         pool: myfs-data0
#
#         # Root path of an existing CephFS volume
#         # Required for provisionVolume: "false"
#         # rootPath: /absolute/path
#
#         # The secrets contain Ceph admin credentials. These are generated automatically by the operator
#         # in the same namespace as the cluster.
#         csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
#         csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
#         csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
#         csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
#
#         # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
#         # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
#         # or by setting the default mounter explicitly via --volumemounter command-line argument.
#         # mounter: kernel
#       reclaimPolicy: Delete
#       mountOptions:
#         # uncomment the following line for debugging
#         #- debug

- name: Create Rook-Ceph StorageClass CRD (Provisioner ceph.rook.io/block)
  copy:
    dest: /home/ubuntu/crd-rook-ceph-storageclass.yml
    mode: 0640
    owner: ubuntu
    content: |
      apiVersion: ceph.rook.io/v1
      kind: CephBlockPool
      metadata:
        name: replicapool
        namespace: rook-ceph
      spec:
        replicated:
          size: 3
      ---
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
         name: rook-ceph-block
      provisioner: ceph.rook.io/block
      parameters:
        blockPool: replicapool
        clusterNamespace: rook-ceph
        fstype: xfs

- name: Rook-Ceph Deploy StorageClass CRD
  become: true
  become_user: ubuntu
  shell: kubectl apply -f /home/ubuntu/crd-rook-ceph-storageclass.yml --dry-run=true -o yaml | kubectl apply -f -
  args:
    chdir: $HOME

- name: Rook-Ceph Mark Storage Class as Default
  become: true
  become_user: ubuntu
  shell: kubectl patch storageclass rook-ceph-block -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' --dry-run=true -o yaml | kubectl apply -f -
  args:
    chdir: $HOME

# - name: Rook-Ceph Deploy Toolbox
#   become: true
#   become_user: ubuntu
#   shell: kubectl apply -f /home/ubuntu/rook/cluster/examples/kubernetes/ceph/toolbox.yaml --dry-run=true -o yaml | kubectl apply -f -
#   args:
#     chdir: $HOME

- name: Create Rook-Ceph DashBoard Nodeport CRD
  copy:
    dest: /home/ubuntu/crd-rook-ceph-dashboard.yml
    mode: 0640
    owner: ubuntu
    content: |
      apiVersion: v1
      kind: Service
      metadata:
        name: rook-ceph-mgr-dashboard-external-https
        namespace: rook-ceph
        labels:
          app: rook-ceph-mgr
          rook_cluster: rook-ceph
      spec:
        ports:
        - name: dashboard
          port: 8443
          protocol: TCP
          targetPort: 8443
          nodePort: {{ rook_ceph_dashboard_nodeport }}
        selector:
          app: rook-ceph-mgr
          rook_cluster: rook-ceph
        sessionAffinity: None
        type: NodePort

- name: Rook-Ceph Deploy Rook-Ceph DashBoard Nodeport CRD
  become: true
  become_user: ubuntu
  shell: kubectl apply -f /home/ubuntu/crd-rook-ceph-dashboard.yml --dry-run=true -o yaml | kubectl apply -f -
  args:
    chdir: $HOME
